# GuÃ­a Completa de ConfiguraciÃ³n de Datadog

## ðŸ“Š Dashboards Disponibles

He creado 3 dashboards principales en formato JSON que puedes importar directamente en Datadog:

### 1. **Microservices Overview - JVM Metrics**

Dashboard enfocado en mÃ©tricas de la JVM para todos los microservicios.

**Widgets incluidos:**
- JVM Heap Memory Usage by Service (lÃ­nea temporal)
- JVM Heap Memory % by Service (valor actual)
- GC Pause Time P95 (barras)
- GC Collections per Minute (lÃ­nea)
- Thread Count by Service (Ã¡rea)
- Classes Loaded (lÃ­nea)
- CPU Usage by Service (top list)
- Non-Heap Memory - Metaspace, Code Cache (Ã¡rea)

**MÃ©tricas clave:**
```
jvm.memory.used{memory:heap}
jvm.memory.max{memory:heap}
jvm.gc.pause
jvm.threads.live
jvm.classes.loaded
process.runtime.jvm.cpu.utilization
```

### 2. **Kafka Metrics Dashboard**

Dashboard especializado en mÃ©tricas de Kafka (producers y consumers).

**Widgets incluidos:**
- Kafka Consumer Lag by Service (lÃ­nea con threshold)
- Messages Consumed Rate (barras)
- Messages Produced Rate (barras)
- Consumer Processing Time P95 (lÃ­nea)
- Consumer Errors (barras)
- Producer Request Rate (valor)
- Consumer Lag Heatmap
- Consumer Groups Status (check status)

**MÃ©tricas clave:**
```
kafka.consumer.lag
kafka.consumer.records.consumed
kafka.producer.record.send.total
spring.kafka.listener.seconds
kafka.consumer.errors
```

### 3. **Business Metrics - Crypto & News**

Dashboard de mÃ©tricas de negocio especÃ­ficas del sistema.

**Widgets incluidos:**
- HTTP Requests per Minute by Service (barras)
- HTTP Request Latency P95 (lÃ­nea)
- HTTP Error Rate 4xx + 5xx (barras)
- Crypto Prices Fetched (valor)
- Price Alerts Triggered (valor)
- News Requests Processed (barras)
- Redis Cache Hit Rate (valor %)
- Redis Connections (lÃ­nea)
- Top 10 Slowest Endpoints (top list)
- Request Success Rate (valor con colores)

**MÃ©tricas clave:**
```
http.server.requests
kafka.producer.record.send.total{topic:crypto-prices}
kafka.producer.record.send.total{topic:price-alerts}
redis.keyspace.hits
redis.keyspace.misses
redis.net.clients
```

## ðŸš¨ Alertas Configuradas

He creado 8 monitores crÃ­ticos en `datadog-alerts.json`:

### Alerta 1: High Error Rate
- **Trigger**: Error rate > 10 errors/sec (5 errors/sec warning)
- **Window**: Last 5 minutes
- **Notifica**: Slack, PagerDuty
- **Prioridad**: High

### Alerta 2: Consumer Lag Too High âš ï¸
- **Trigger**: Lag > 1000 messages (500 warning)
- **Window**: Last 10 minutes
- **Notifica**: Slack Kafka channel, On-call
- **Prioridad**: Critical

### Alerta 3: Memory Usage Above 90% ðŸ”´
- **Trigger**: Heap > 90% (80% warning)
- **Window**: Last 15 minutes
- **Notifica**: Slack, On-call, PagerDuty
- **Prioridad**: Critical

### Alerta 4: Slow API Response Time
- **Trigger**: P95 latency > 2 seconds (1 second warning)
- **Window**: Last 10 minutes
- **Notifica**: Slack performance channel
- **Prioridad**: Medium

### Alerta 5: Service Down ðŸ”´
- **Trigger**: Health check failing
- **Check**: Last 2 checks
- **Notifica**: Slack critical, PagerDuty, On-call
- **Prioridad**: Critical

### Alerta 6: High Redis Connection Count
- **Trigger**: Connections > 100 (75 warning)
- **Window**: Last 10 minutes
- **Prioridad**: Medium

### Alerta 7: Excessive Garbage Collection
- **Trigger**: GC rate > 10/sec (5/sec warning)
- **Window**: Last 15 minutes
- **Prioridad**: Medium

### Alerta 8: No Crypto Prices Fetched
- **Trigger**: No messages in last 30 minutes
- **Window**: Last 30 minutes
- **Prioridad**: High

## ðŸ—ºï¸ Service Map - Arquitectura del Sistema

### **CÃ³mo Acceder al Service Map**

1. Ve a: https://app.datadoghq.com/apm/map
2. Filtra por `env:docker-local`
3. Selecciona el tiempo: Last 1 hour

### **Arquitectura Visualizada**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SERVICE MAP                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Client     â”‚
                          â”‚  (Browser)   â”‚
                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ HTTP GET
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      news-api            â”‚
                    â”‚      Port: 8080          â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                    â”‚  â”‚  Endpoints:      â”‚    â”‚
                    â”‚  â”‚  /api/v1/news    â”‚    â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                         â”‚              â”‚
              Kafka Pub  â”‚              â”‚ Redis Cache
              Topic:news â”‚              â”‚ Read/Write
                         â”‚              â”‚
                         â”‚         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚         â”‚   Redis     â”‚
                         â”‚         â”‚   Port:6379 â”‚
                         â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Apache Kafka      â”‚
              â”‚   Port: 29092       â”‚
              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
              â”‚  â”‚ Topics:      â”‚   â”‚
              â”‚  â”‚ - news       â”‚   â”‚
              â”‚  â”‚ - crypto-    â”‚   â”‚
              â”‚  â”‚   prices     â”‚   â”‚
              â”‚  â”‚ - price-     â”‚   â”‚
              â”‚  â”‚   alerts     â”‚   â”‚
              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
              â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                 â”‚         â”‚    â”‚
    Kafka Sub   â”‚         â”‚    â”‚
                 â”‚         â”‚    â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ worker-     â”‚  â”‚ crypto-fetcher-  â”‚  â”‚ price-         â”‚
      â”‚ service     â”‚  â”‚ service          â”‚  â”‚ processor-     â”‚
      â”‚ Port: 8081  â”‚  â”‚ Port: 8083       â”‚  â”‚ service        â”‚
      â”‚             â”‚  â”‚                  â”‚  â”‚ Port: 8084     â”‚
      â”‚ Consumes:   â”‚  â”‚ Produces:        â”‚  â”‚                â”‚
      â”‚ - news      â”‚  â”‚ - crypto-prices  â”‚  â”‚ Consumes:      â”‚
      â”‚             â”‚  â”‚                  â”‚  â”‚ - crypto-pricesâ”‚
      â”‚ Publishes:  â”‚  â”‚ External API:    â”‚  â”‚                â”‚
      â”‚ Redis cache â”‚  â”‚ CoinCap API      â”‚  â”‚ Produces:      â”‚
      â”‚             â”‚  â”‚                  â”‚  â”‚ Redis cache    â”‚
      â”‚ Calls:      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ MediaStack  â”‚                                  â”‚
      â”‚ API         â”‚                                  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    Kafka Pub     â”‚
                                        Topic: crypto-prices
                                                       â”‚
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚   alert-service    â”‚
                                          â”‚   Port: 8085       â”‚
                                          â”‚                    â”‚
                                          â”‚   Consumes:        â”‚
                                          â”‚   - crypto-prices  â”‚
                                          â”‚                    â”‚
                                          â”‚   Detects >5%      â”‚
                                          â”‚   price changes    â”‚
                                          â”‚                    â”‚
                                          â”‚   Produces:        â”‚
                                          â”‚   - price-alerts   â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Flujos Principales**

#### Flow 1: News Request
```
Client â†’ news-api â†’ Redis (check cache)
                 â”œâ”€ Cache Hit â†’ Return data (200)
                 â””â”€ Cache Miss â†’ Publish to Kafka topic "news" (404)
                              â†’ worker-service consumes message
                              â†’ Fetch from MediaStack API
                              â†’ Store in Redis
```

#### Flow 2: Crypto Price Processing
```
crypto-fetcher-service (scheduled) â†’ Fetch from CoinCap API
                                  â†’ Publish to Kafka "crypto-prices"
                                  â†’ price-processor-service consumes
                                  â†’ Store in Redis
                                  â†’ alert-service consumes
                                  â†’ Detect price changes >5%
                                  â†’ Publish to Kafka "price-alerts"
```

### **MÃ©tricas Clave por Flujo**

**News Flow:**
- `http.server.requests{service:news-api,uri:/api/v1/news}`
- `kafka.producer.record.send.total{topic:news}`
- `kafka.consumer.lag{service:worker-service,topic:news}`
- `redis.keyspace.hits` vs `redis.keyspace.misses`

**Crypto Price Flow:**
- `kafka.producer.record.send.total{service:crypto-fetcher-service,topic:crypto-prices}`
- `kafka.consumer.lag{service:price-processor-service,topic:crypto-prices}`
- `kafka.consumer.lag{service:alert-service,topic:crypto-prices}`
- `kafka.producer.record.send.total{service:alert-service,topic:price-alerts}`

## ðŸ“¥ Importar Dashboards y Alertas

### OpciÃ³n 1: Via UI (Recomendado)

**Dashboards:**
1. Ve a: https://app.datadoghq.com/dashboard/lists
2. Click "New Dashboard" â†’ "Import Dashboard"
3. Copia el contenido de `datadog-dashboards.json`
4. Pega en el editor
5. Click "Save"
6. Repite para cada dashboard (JVM, Kafka, Business)

**Alertas:**
1. Ve a: https://app.datadoghq.com/monitors/manage
2. Click "New Monitor"
3. Selecciona el tipo segÃºn el alert
4. Copia la configuraciÃ³n de `datadog-alerts.json`
5. Ajusta notification channels (@slack, @pagerduty)
6. Click "Create"
7. Repite para cada monitor

### OpciÃ³n 2: Via API

```bash
# Importar dashboard
curl -X POST "https://api.datadoghq.com/api/v1/dashboard" \
  -H "Content-Type: application/json" \
  -H "DD-API-KEY: ${DD_API_KEY}" \
  -H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
  -d @datadog-dashboards.json

# Importar monitor/alert
curl -X POST "https://api.datadoghq.com/api/v1/monitor" \
  -H "Content-Type: application/json" \
  -H "DD-API-KEY: ${DD_API_KEY}" \
  -H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
  -d @datadog-alerts.json
```

### OpciÃ³n 3: Via Terraform (Infraestructura como CÃ³digo)

```hcl
# datadog_monitors.tf
resource "datadog_monitor" "high_error_rate" {
  name    = "[Microservices] High Error Rate"
  type    = "metric alert"
  message = "..."
  query   = "sum(last_5m):sum:http.server.requests{status:5*}.as_rate() by {service} > 10"

  monitor_thresholds {
    critical = 10
    warning  = 5
  }

  tags = ["env:docker-local", "team:backend"]
}
```

## ðŸ”” Configurar Notificaciones

### Slack Integration

1. Ve a: https://app.datadoghq.com/account/settings#integrations/slack
2. Click "Add Slack Account"
3. Autoriza Datadog en tu workspace
4. Configura channels:
   - `#alerts` â†’ @slack-alerts
   - `#kafka-alerts` â†’ @slack-kafka
   - `#performance` â†’ @slack-performance
   - `#critical` â†’ @slack-critical

### PagerDuty Integration

1. Ve a: https://app.datadoghq.com/account/settings#integrations/pagerduty
2. AÃ±ade tu PagerDuty API key
3. Configura escalation policies
4. Mapea servicios a on-call schedules

## ðŸ“Š MÃ©tricas Disponibles en APM

El Datadog Java Agent (dd-java-agent.jar) automÃ¡ticamente instrumenta:

### HTTP Requests
- Request rate, latency, errors
- Endpoint breakdown
- Status code distribution

### Kafka
- Producer/Consumer metrics
- Lag monitoring
- Throughput

### Database (Redis)
- Query performance
- Connection pooling
- Cache hit rate

### JVM
- Heap/Non-heap memory
- GC metrics
- Thread pools
- Class loading

## ðŸŽ¯ KPIs Recomendados

### Availability
- **Service Uptime**: > 99.9%
- **Health Check Success Rate**: > 99.5%

### Performance
- **API Response Time (P95)**: < 500ms
- **Kafka Consumer Lag**: < 100 messages
- **Redis Cache Hit Rate**: > 80%

### Reliability
- **Error Rate**: < 1%
- **Success Rate**: > 99%
- **GC Pause Time**: < 100ms

### Resource Usage
- **JVM Heap Usage**: < 80%
- **CPU Usage**: < 70%
- **Thread Count**: Stable

## ðŸš€ PrÃ³ximos Pasos

1. âœ… Importar dashboards
2. âœ… Configurar alertas
3. âœ… Setup Slack notifications
4. ðŸ”„ Configurar PagerDuty (opcional)
5. ðŸ”„ Crear custom metrics para business logic
6. ðŸ”„ Setup SLOs (Service Level Objectives)
7. ðŸ”„ Configurar Synthetic tests para endpoints crÃ­ticos
8. ðŸ”„ Habilitar Continuous Profiler

## ðŸ“š Recursos Adicionales

- [Datadog APM Documentation](https://docs.datadoghq.com/tracing/)
- [Java APM Best Practices](https://docs.datadoghq.com/tracing/setup_overview/setup/java/)
- [Dashboard Best Practices](https://docs.datadoghq.com/dashboards/guide/best-practices/)
- [Monitor Best Practices](https://docs.datadoghq.com/monitors/guide/best-practices/)
